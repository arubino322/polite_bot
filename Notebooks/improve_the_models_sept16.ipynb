{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import codecs\n",
    "import io\n",
    "import pickle\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#Extracting features from text, define target y and data X\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's improve the models.\n",
    "\n",
    "We have three models that we want to improve. An idea for cleaning the MNB:\n",
    "- apply the nltk TweetTokenizer function to see if this tokenization method improves model.\n",
    "- use this method of tokenizing for every model.\n",
    "- apply gridsearch to each model.\n",
    "- use the Pipeline feature and repickle each model with the pipeline applied.\n",
    "\n",
    "THEN I'll feel comfortable applying this to unseen data.\n",
    "\n",
    "What if we read in all the raw files that I originally used to put together the master file, and then applied the tokenization AFTER combining them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10662\n",
      "(10662, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9145</th>\n",
       "      <td>williams plays sy , another of his open-faced ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>short_movie_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7422</th>\n",
       "      <td>this delicately observed story , deeply felt a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>short_movie_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10244</th>\n",
       "      <td>expect the same-old , lame-old slasher nonsens...</td>\n",
       "      <td>negative</td>\n",
       "      <td>short_movie_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>the story that emerges has elements of romance...</td>\n",
       "      <td>positive</td>\n",
       "      <td>short_movie_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8612</th>\n",
       "      <td>tom green and an ivy league college should nev...</td>\n",
       "      <td>negative</td>\n",
       "      <td>short_movie_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>despite terrific special effects and funnier g...</td>\n",
       "      <td>negative</td>\n",
       "      <td>short_movie_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8567</th>\n",
       "      <td>let's cut to the consumer-advice bottom line :...</td>\n",
       "      <td>negative</td>\n",
       "      <td>short_movie_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9045</th>\n",
       "      <td>it made me want to wrench my eyes out of my he...</td>\n",
       "      <td>negative</td>\n",
       "      <td>short_movie_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3949</th>\n",
       "      <td>no , it's not as single-minded as john carpent...</td>\n",
       "      <td>positive</td>\n",
       "      <td>short_movie_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380</th>\n",
       "      <td>it's clear why deuces wild , which was shot tw...</td>\n",
       "      <td>negative</td>\n",
       "      <td>short_movie_reviews</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment  \\\n",
       "9145   williams plays sy , another of his open-faced ...  positive   \n",
       "7422   this delicately observed story , deeply felt a...  positive   \n",
       "10244  expect the same-old , lame-old slasher nonsens...  negative   \n",
       "2150   the story that emerges has elements of romance...  positive   \n",
       "8612   tom green and an ivy league college should nev...  negative   \n",
       "1312   despite terrific special effects and funnier g...  negative   \n",
       "8567   let's cut to the consumer-advice bottom line :...  negative   \n",
       "9045   it made me want to wrench my eyes out of my he...  negative   \n",
       "3949   no , it's not as single-minded as john carpent...  positive   \n",
       "5380   it's clear why deuces wild , which was shot tw...  negative   \n",
       "\n",
       "                    source  \n",
       "9145   short_movie_reviews  \n",
       "7422   short_movie_reviews  \n",
       "10244  short_movie_reviews  \n",
       "2150   short_movie_reviews  \n",
       "8612   short_movie_reviews  \n",
       "1312   short_movie_reviews  \n",
       "8567   short_movie_reviews  \n",
       "9045   short_movie_reviews  \n",
       "3949   short_movie_reviews  \n",
       "5380   short_movie_reviews  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform\n",
    "\n",
    "short_pos = io.open(\"../data/training_data/short_positive_movie_reviews.txt\", \"r\", encoding=\"latin-1\").read()\n",
    "short_neg = io.open(\"../data/training_data/short_negative_movie_reviews.txt\", \"r\", encoding=\"latin-1\").read()\n",
    "\n",
    "all_reviews = []\n",
    "\n",
    "for i in short_pos.split('\\n'):\n",
    "    all_reviews.append((i, \"positive\"))\n",
    "    \n",
    "for i in short_neg.split('\\n'):\n",
    "    all_reviews.append((i, \"negative\"))\n",
    "    \n",
    "print(len(all_reviews))\n",
    "\n",
    "smr_df = pd.DataFrame(all_reviews, columns=['text', 'sentiment'])\n",
    "# shuffle dataframe\n",
    "smr_df = smr_df.sample(frac=1).reset_index(drop=True)\n",
    "smr_df['source'] = 'short_movie_reviews'\n",
    "print(smr_df.shape)\n",
    "smr_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    5331\n",
       "positive    5331\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smr_df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # let's do the damn thang\n",
    "# # create a function, maybe add a parameter to account for max_features\n",
    "\n",
    "# def sklearn_naive_bayes(train, cat, n_features):\n",
    "#     vect = TfidfVectorizer(ngram_range=(1, 2), max_features=n_features)\n",
    "#     X = vect.fit_transform(train)\n",
    "#     y = cat\n",
    "\n",
    "#     #Partitioning the data into test and training set\n",
    "#     SPLIT_PERC = 0.75\n",
    "#     split_size = int(len(y)*SPLIT_PERC)\n",
    "\n",
    "#     X_train = X[:split_size]\n",
    "#     X_test = X[split_size:]\n",
    "#     y_train = y[:split_size]\n",
    "#     y_test = y[split_size:]\n",
    "\n",
    "#     #Training the model\n",
    "#     clf = MultinomialNB()\n",
    "#     clf.fit(X_train, y_train)\n",
    "\n",
    "#     #Evaluating the results\n",
    "#     print(\"Accuracy on training set:\")\n",
    "#     print(clf.score(X_train, y_train))\n",
    "#     print(\"Accuracy on testing set:\")\n",
    "#     print(clf.score(X_test, y_test))\n",
    "#     y_pred = clf.predict(X_test)\n",
    "#     print(\"Classification Report:\")\n",
    "#     print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK\n",
    "Note that I'm only using unigrams for NLTK cuz I haven't figured out how to use bigrams with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nltk_naive_bayes(df, n_features):\n",
    "\n",
    "#     # split the cleaned_tweets column\n",
    "#     df['split_tweets'] = df.cleaned_tweets.apply(lambda x: x.split())\n",
    "#     all_reviews = zip(df.split_tweets, df.sentiment)\n",
    "\n",
    "#     all_tweets = ' '.join(df['cleaned_tweets']).split()\n",
    "\n",
    "#     freq_tweets = nltk.FreqDist(all_tweets)\n",
    "#     print('The top 50 most common words are:')\n",
    "#     print(freq_tweets.most_common(50))\n",
    "\n",
    "#     word_features = [w[0] for w in freq_tweets.most_common(n_features)]\n",
    "#     # as we can see, there are a lot of stop words in this list. we should consider\n",
    "#     # removing the stop words to get better word features, but for now let's leave them in.\n",
    "    \n",
    "#     def find_features(document):\n",
    "#         words = set(document)\n",
    "#         features = {}\n",
    "#         for w in word_features:\n",
    "#             features[w] = (w in words)\n",
    "\n",
    "#         return features\n",
    "\n",
    "#     featuresets = [(find_features(rev), category) for (rev, category) in all_reviews]\n",
    "\n",
    "#     training_set = featuresets[:int(len(featuresets)*.75)]\n",
    "#     testing_set = featuresets[int(len(featuresets)*.75):]\n",
    "\n",
    "\n",
    "#     # This is the algorithm calculation:\n",
    "#     # posterior = prior occurences x likelihood / evidence\n",
    "#     # the above gives us the likelihood of something to be positive (or negative). It's not the best algorithm,\n",
    "#     # but it's scalable and easy to use\n",
    "\n",
    "#     classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "#     print(\"Naive Bayes Algo accuracy:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "#     classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GREAT! OK. Let's make some functions so we can reproduce these steps with the other datasets\n",
    "\n",
    "It might be worth doing some sort of gridsearch to find the point of diminishing returns for number of features to include in your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airline reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n",
      "       'negativereason', 'negativereason_confidence', 'airline',\n",
      "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
      "       'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n",
      "       'tweet_location', 'user_timezone'],\n",
      "      dtype='object')\n",
      "(14640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10849</th>\n",
       "      <td>@USAirways Thanks. No DC yet, I see. I will ke...</td>\n",
       "      <td>positive</td>\n",
       "      <td>airline_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4782</th>\n",
       "      <td>@SouthwestAir   Are flights going into Dallas ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>airline_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13888</th>\n",
       "      <td>@AmericanAir @USAirways how can you have no fo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>airline_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2871</th>\n",
       "      <td>@united Still waiting on our bag! Never got de...</td>\n",
       "      <td>negative</td>\n",
       "      <td>airline_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5728</th>\n",
       "      <td>@SouthwestAir... I love you. Air travel doesn'...</td>\n",
       "      <td>positive</td>\n",
       "      <td>airline_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11135</th>\n",
       "      <td>@USAirways we bought our tickets months ago. H...</td>\n",
       "      <td>negative</td>\n",
       "      <td>airline_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12448</th>\n",
       "      <td>@AmericanAir I'm flying into DCA my bag is at ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>airline_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>@VirginAmerica you will match my #AmericanAirl...</td>\n",
       "      <td>positive</td>\n",
       "      <td>airline_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13267</th>\n",
       "      <td>@AmericanAir Can you add my KTN to an existing...</td>\n",
       "      <td>negative</td>\n",
       "      <td>airline_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6448</th>\n",
       "      <td>@SouthwestAir I hope you're happy! You have of...</td>\n",
       "      <td>negative</td>\n",
       "      <td>airline_reviews</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment  \\\n",
       "10849  @USAirways Thanks. No DC yet, I see. I will ke...  positive   \n",
       "4782   @SouthwestAir   Are flights going into Dallas ...   neutral   \n",
       "13888  @AmericanAir @USAirways how can you have no fo...  negative   \n",
       "2871   @united Still waiting on our bag! Never got de...  negative   \n",
       "5728   @SouthwestAir... I love you. Air travel doesn'...  positive   \n",
       "11135  @USAirways we bought our tickets months ago. H...  negative   \n",
       "12448  @AmericanAir I'm flying into DCA my bag is at ...  negative   \n",
       "189    @VirginAmerica you will match my #AmericanAirl...  positive   \n",
       "13267  @AmericanAir Can you add my KTN to an existing...  negative   \n",
       "6448   @SouthwestAir I hope you're happy! You have of...  negative   \n",
       "\n",
       "                source  \n",
       "10849  airline_reviews  \n",
       "4782   airline_reviews  \n",
       "13888  airline_reviews  \n",
       "2871   airline_reviews  \n",
       "5728   airline_reviews  \n",
       "11135  airline_reviews  \n",
       "12448  airline_reviews  \n",
       "189    airline_reviews  \n",
       "13267  airline_reviews  \n",
       "6448   airline_reviews  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's take a look at another dataset that we might be able to train with\n",
    "airlines = pd.read_csv('../data/training_data/airline_reviews.csv')\n",
    "print(airlines.columns)\n",
    "airlines_df = airlines[['text', 'airline_sentiment']]\n",
    "airlines_df.columns = ['text', 'sentiment']\n",
    "airlines_df['source'] = 'airline_reviews'\n",
    "print(airlines_df.shape)\n",
    "airlines_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clean tweet with function i found online\n",
    "# def clean_tweet(tweet): \n",
    "#     ''' \n",
    "#     Utility function to clean tweet text by removing links, special characters \n",
    "#     using simple regex statements. \n",
    "#     '''\n",
    "#     x = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) | (\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "#     cleaned = re.sub(r'[^\\x00-\\x7f]',r'', re.sub(r'https:\\/\\/t.co\\/[A-z0-9]*', '', x))\n",
    "#     #take out the hashtags\n",
    "#     without_hashtags = re.sub(r'[^\\x00-\\x7f]',r'', re.sub(r'\\#[A-z0-9]*', '', cleaned))\n",
    "#     # or you can remove it by applying: .decode('utf8').encode('ascii', errors='ignore')\n",
    "#     return without_hashtags\n",
    "\n",
    "# # tokenize and shit\n",
    "# def tokenize_and_stem(row):\n",
    "    \n",
    "#     # remove punctuations\n",
    "#     i = re.sub(r'[^\\w\\s]','',row)    \n",
    "#     # tokenize the words in the short reviews first\n",
    "#     words = word_tokenize(i)\n",
    "#     # remove stop words\n",
    "#     filtered = [ps.stem(w) for w in words if not w in stop_words]\n",
    "#     return ' '.join(filtered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>Sadly, Gordon Ramsey's Steak is a place we sha...</td>\n",
       "      <td>negative</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>I ate there twice on my last visit, and especi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>On three different occasions I asked for well ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>I could barely stomach the meal, but didn't co...</td>\n",
       "      <td>negative</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>It's NOT hard to make a decent hamburger.</td>\n",
       "      <td>negative</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>Cant say enough good things about this place.</td>\n",
       "      <td>positive</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>like the other reviewer said \"you couldn't pay...</td>\n",
       "      <td>negative</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>Overall, a great experience.</td>\n",
       "      <td>positive</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>We ordered the duck rare and it was pink and t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>The decor is nice, and the piano music soundtr...</td>\n",
       "      <td>positive</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text sentiment source\n",
       "356  Sadly, Gordon Ramsey's Steak is a place we sha...  negative   yelp\n",
       "842  I ate there twice on my last visit, and especi...  positive   yelp\n",
       "884  On three different occasions I asked for well ...  negative   yelp\n",
       "925  I could barely stomach the meal, but didn't co...  negative   yelp\n",
       "965          It's NOT hard to make a decent hamburger.  negative   yelp\n",
       "502      Cant say enough good things about this place.  positive   yelp\n",
       "244  like the other reviewer said \"you couldn't pay...  negative   yelp\n",
       "899                       Overall, a great experience.  positive   yelp\n",
       "50   We ordered the duck rare and it was pink and t...  positive   yelp\n",
       "687  The decor is nice, and the piano music soundtr...  positive   yelp"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp = pd.read_csv('../data/training_data/yelp_labelled.txt', sep=\"\\t\", header=None)\n",
    "yelp.columns = ['text', 'category']\n",
    "yelp['source'] = 'yelp'\n",
    "yelp['sentiment'] = np.where(yelp.category == 1, 'positive', 'negative')\n",
    "yelp_df = yelp[['text', 'sentiment', 'source']]\n",
    "yelp_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    500\n",
       "negative    500\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>Not a good item.. It worked for a while then s...</td>\n",
       "      <td>negative</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>The earpiece on this is too large or too heavy...</td>\n",
       "      <td>negative</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>That's a huge design flaw (unless I'm not usin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>Perhaps my phone is defective, but people cann...</td>\n",
       "      <td>negative</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>It was an inexpensive piece, but I would still...</td>\n",
       "      <td>negative</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Buyer--Be Very Careful!!!!!.</td>\n",
       "      <td>negative</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>I have tried these cables with my computer and...</td>\n",
       "      <td>positive</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>This is hands down the best phone I've ever had.</td>\n",
       "      <td>positive</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>So far it has worked like a charm.</td>\n",
       "      <td>positive</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>Excellent Phone.</td>\n",
       "      <td>positive</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text sentiment  source\n",
       "374  Not a good item.. It worked for a while then s...  negative  amazon\n",
       "652  The earpiece on this is too large or too heavy...  negative  amazon\n",
       "106  That's a huge design flaw (unless I'm not usin...  negative  amazon\n",
       "542  Perhaps my phone is defective, but people cann...  negative  amazon\n",
       "338  It was an inexpensive piece, but I would still...  negative  amazon\n",
       "134                       Buyer--Be Very Careful!!!!!.  negative  amazon\n",
       "941  I have tried these cables with my computer and...  positive  amazon\n",
       "295   This is hands down the best phone I've ever had.  positive  amazon\n",
       "862                 So far it has worked like a charm.  positive  amazon\n",
       "292                                   Excellent Phone.  positive  amazon"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon = pd.read_csv('../data/training_data/amazon_cells_labelled.txt', \n",
    "                   sep=\"\\t\", header=None)\n",
    "amazon.columns = ['text', 'category']\n",
    "amazon['sentiment'] = np.where(amazon.category == 1, 'positive', 'negative')\n",
    "amazon['source'] = 'amazon'\n",
    "amazon_df = amazon[['text', 'sentiment', 'source']]\n",
    "print(amazon_df.shape)\n",
    "amazon_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    500\n",
       "negative    500\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford dataset\n",
    "\n",
    "- 0: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "- 1: the id of the tweet (2087)\n",
    "- 2: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "- 3: the query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "- 4: the user that tweeted (robotickilldozr)\n",
    "- 5: the text of the tweet (Lyx is cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>736874</th>\n",
       "      <td>Trying to study for the upcoming exams but I a...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518222</th>\n",
       "      <td>I think i'm slowly beginning to cheer up. It's...</td>\n",
       "      <td>positive</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923037</th>\n",
       "      <td>TODAY ONLY! - 20% off everything for Mother's ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664937</th>\n",
       "      <td>If I'd waited ONE and a HALF minutes to leave ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178545</th>\n",
       "      <td>in a very stress mood</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557273</th>\n",
       "      <td>&amp;quot;When you came in the air went out, and e...</td>\n",
       "      <td>positive</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210243</th>\n",
       "      <td>Good night... What a day! What a party!</td>\n",
       "      <td>positive</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328334</th>\n",
       "      <td>is sick of having bad days at work. I don't kn...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115383</th>\n",
       "      <td>In Alexandria. Almost home  So much to do befo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281759</th>\n",
       "      <td>@BrandyWandLover dammit, can't play it on this...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text sentiment    source\n",
       "736874   Trying to study for the upcoming exams but I a...  negative  stanford\n",
       "1518222  I think i'm slowly beginning to cheer up. It's...  positive  stanford\n",
       "923037   TODAY ONLY! - 20% off everything for Mother's ...  positive  stanford\n",
       "664937   If I'd waited ONE and a HALF minutes to leave ...  negative  stanford\n",
       "178545                              in a very stress mood   negative  stanford\n",
       "1557273  &quot;When you came in the air went out, and e...  positive  stanford\n",
       "1210243           Good night... What a day! What a party!   positive  stanford\n",
       "328334   is sick of having bad days at work. I don't kn...  negative  stanford\n",
       "115383   In Alexandria. Almost home  So much to do befo...  negative  stanford\n",
       "281759   @BrandyWandLover dammit, can't play it on this...  negative  stanford"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and then this random training set from airlines\n",
    "stanford = pd.read_csv('../data/training_data/stanford_training_labelled.csv', header=None, encoding='ISO-8859-1')\n",
    "stanford.columns = ['polarity', 'tweet_id', 'date', 'query', 'user', 'text']\n",
    "stanford['sentiment'] = np.where(stanford.polarity == 0, 'negative', \n",
    "                                 np.where(stanford.polarity == 2, 'neutral', 'positive'))\n",
    "stanford['source'] = 'stanford'\n",
    "stanford_df = stanford[['text', 'sentiment', 'source']]\n",
    "print(stanford_df.shape)\n",
    "stanford_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    800000\n",
       "negative    800000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stanford_df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Need a hug</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spring break in plain city... it's snowing</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I just re-pierced my ears</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@caregiving I couldn't bear to watch it.  And ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@octolinz16 It it counts, idk why I did either...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@smarrison i would've been the first, but i di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@iamjazzyfizzle I wish I got to watch it with ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hollis' death scene will hurt me severely to w...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>about to file taxes</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@LettyA ahh ive always wanted to see rent  lov...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@FakerPattyPattz Oh dear. Were you drinking ou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>@alydesigns i was out most of the day so didn'...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>one of my friend called me, and asked to meet ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>@angry_barista I baked you a cake but I ated it</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>this week is not going as i had hoped</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>blagh class at 8 tomorrow</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text sentiment    source\n",
       "0   @switchfoot http://twitpic.com/2y1zl - Awww, t...  negative  stanford\n",
       "1   is upset that he can't update his Facebook by ...  negative  stanford\n",
       "2   @Kenichan I dived many times for the ball. Man...  negative  stanford\n",
       "3     my whole body feels itchy and like its on fire   negative  stanford\n",
       "4   @nationwideclass no, it's not behaving at all....  negative  stanford\n",
       "5                       @Kwesidei not the whole crew   negative  stanford\n",
       "6                                         Need a hug   negative  stanford\n",
       "7   @LOLTrish hey  long time no see! Yes.. Rains a...  negative  stanford\n",
       "8                @Tatiana_K nope they didn't have it   negative  stanford\n",
       "9                           @twittera que me muera ?   negative  stanford\n",
       "10        spring break in plain city... it's snowing   negative  stanford\n",
       "11                         I just re-pierced my ears   negative  stanford\n",
       "12  @caregiving I couldn't bear to watch it.  And ...  negative  stanford\n",
       "13  @octolinz16 It it counts, idk why I did either...  negative  stanford\n",
       "14  @smarrison i would've been the first, but i di...  negative  stanford\n",
       "15  @iamjazzyfizzle I wish I got to watch it with ...  negative  stanford\n",
       "16  Hollis' death scene will hurt me severely to w...  negative  stanford\n",
       "17                               about to file taxes   negative  stanford\n",
       "18  @LettyA ahh ive always wanted to see rent  lov...  negative  stanford\n",
       "19  @FakerPattyPattz Oh dear. Were you drinking ou...  negative  stanford\n",
       "20  @alydesigns i was out most of the day so didn'...  negative  stanford\n",
       "21  one of my friend called me, and asked to meet ...  negative  stanford\n",
       "22   @angry_barista I baked you a cake but I ated it   negative  stanford\n",
       "23             this week is not going as i had hoped   negative  stanford\n",
       "24                         blagh class at 8 tomorrow   negative  stanford"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stanford_df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.000123, which corresponds to a distance of 705 Mly, or 216 Mpc',\n",
       " '000webhost is a free web hosting service, operated by Hostinger',\n",
       " '0010x0010 is a Dutch-born audiovisual artist, currently living in Los Angeles',\n",
       " '0-0-1-3 is an alcohol abuse prevention program developed in 2004 at Francis E. Warren Air Force Base based on research by the National Institute on Alcohol Abuse and Alcoholism regarding binge drinking in college students',\n",
       " '0.01 is the debut studio album of H3llb3nt, released on February 20, 1996 by Fifth Colvmn Records',\n",
       " '001 of 3 February 1997, which was signed between the Government of the Republic of Rwanda, and FAPADER',\n",
       " '003230 is a South Korean food manufacturer',\n",
       " '0.04%Gas molecules in soil are in continuous thermal motion according to the kinetic theory of gasses, there is also collision between molecules - a random walk',\n",
       " '0.04% of the votes were invalid',\n",
       " '005.1999.06 is the fifth studio album by the South Korean singer and actress Uhm Jung-hwa']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = []\n",
    "\n",
    "with open('../data/wikisent2.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        lines.append(line[:-2])\n",
    "\n",
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7871825, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000123, which corresponds to a distance of 7...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000webhost is a free web hosting service, oper...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0010x0010 is a Dutch-born audiovisual artist, ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0-0-1-3 is an alcohol abuse prevention program...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01 is the debut studio album of H3llb3nt, re...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment     source\n",
       "0  0.000123, which corresponds to a distance of 7...   neutral  wikipedia\n",
       "1  000webhost is a free web hosting service, oper...   neutral  wikipedia\n",
       "2  0010x0010 is a Dutch-born audiovisual artist, ...   neutral  wikipedia\n",
       "3  0-0-1-3 is an alcohol abuse prevention program...   neutral  wikipedia\n",
       "4  0.01 is the debut studio album of H3llb3nt, re...   neutral  wikipedia"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df = pd.DataFrame({'text':lines,\n",
    "                        'sentiment':'neutral',\n",
    "                        'source':'wikipedia'})\n",
    "\n",
    "print(wiki_df.shape)\n",
    "wiki_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4017383</th>\n",
       "      <td>McGregor's work was rooted in motivation theor...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3169681</th>\n",
       "      <td>It is the first wuxia television series to fil...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6168424</th>\n",
       "      <td>The Indiana Business Bulletin provides weekly ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4794525</th>\n",
       "      <td>Romania and Moldova are Eastern Orthodox count...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595353</th>\n",
       "      <td>A true claw is made of hard protein called ker...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3503116</th>\n",
       "      <td>It was released in July 1965 by Blue Note Records</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5164280</th>\n",
       "      <td>Some protists are related to animals and some ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677615</th>\n",
       "      <td>Barry Stuart McDonald (born 9 June 1942) was a...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619500</th>\n",
       "      <td>Furth and Sondheim retained the basic structur...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7519991</th>\n",
       "      <td>Trachylepis boettgeri, commonly knowm as Boett...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text sentiment  \\\n",
       "4017383  McGregor's work was rooted in motivation theor...   neutral   \n",
       "3169681  It is the first wuxia television series to fil...   neutral   \n",
       "6168424  The Indiana Business Bulletin provides weekly ...   neutral   \n",
       "4794525  Romania and Moldova are Eastern Orthodox count...   neutral   \n",
       "595353   A true claw is made of hard protein called ker...   neutral   \n",
       "3503116  It was released in July 1965 by Blue Note Records   neutral   \n",
       "5164280  Some protists are related to animals and some ...   neutral   \n",
       "677615   Barry Stuart McDonald (born 9 June 1942) was a...   neutral   \n",
       "1619500  Furth and Sondheim retained the basic structur...   neutral   \n",
       "7519991  Trachylepis boettgeri, commonly knowm as Boett...   neutral   \n",
       "\n",
       "            source  \n",
       "4017383  wikipedia  \n",
       "3169681  wikipedia  \n",
       "6168424  wikipedia  \n",
       "4794525  wikipedia  \n",
       "595353   wikipedia  \n",
       "3503116  wikipedia  \n",
       "5164280  wikipedia  \n",
       "677615   wikipedia  \n",
       "1619500  wikipedia  \n",
       "7519991  wikipedia  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(24)\n",
    "wiki_sample = wiki_df.sample(400000)\n",
    "wiki_sample.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append the all together!\n",
    "\n",
    "Well, let's maybe append part of it together. I don't think we're gonna need all of the stanford dataset, maybe 1/2 of it. The models seem to do pretty good without a ton of data.\n",
    "\n",
    "1. smr_df\n",
    "2. airlines_df\n",
    "3. yelp_df\n",
    "5. amazon_df\n",
    "6. stanford_df (50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1227302, 3)\n",
      "stanford               800000\n",
      "wikipedia              400000\n",
      "airline_reviews         14640\n",
      "short_movie_reviews     10662\n",
      "amazon                   1000\n",
      "yelp                     1000\n",
      "Name: source, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>447463</th>\n",
       "      <td>is chatting on msn to Jennypoo!! &amp;lt;3 Shes li...</td>\n",
       "      <td>positive</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062506</th>\n",
       "      <td>bye for now everyone! it's raining so hard out...</td>\n",
       "      <td>positive</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283133</th>\n",
       "      <td>@ALLIEINCREDIBLE got money on the mind :o) Sle...</td>\n",
       "      <td>positive</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688908</th>\n",
       "      <td>Obama ladislavii is a species of Brazilian lan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686414</th>\n",
       "      <td>My case came open today and dropped my new iPh...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986570</th>\n",
       "      <td>@emilywalkerr @justinlovescolt haha, okay, may...</td>\n",
       "      <td>positive</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899448</th>\n",
       "      <td>Cousin is gone  have had a lovely weekend, bac...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638663</th>\n",
       "      <td>Doon is also part of Shimla Lok Sabha constitu...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737824</th>\n",
       "      <td>wishes she had another day to revise for tomor...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007587</th>\n",
       "      <td>Besides his books, he has written for numerous...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text sentiment  \\\n",
       "447463   is chatting on msn to Jennypoo!! &lt;3 Shes li...  positive   \n",
       "1062506  bye for now everyone! it's raining so hard out...  positive   \n",
       "283133   @ALLIEINCREDIBLE got money on the mind :o) Sle...  positive   \n",
       "688908   Obama ladislavii is a species of Brazilian lan...   neutral   \n",
       "686414   My case came open today and dropped my new iPh...  negative   \n",
       "986570   @emilywalkerr @justinlovescolt haha, okay, may...  positive   \n",
       "899448   Cousin is gone  have had a lovely weekend, bac...  negative   \n",
       "638663   Doon is also part of Shimla Lok Sabha constitu...   neutral   \n",
       "737824   wishes she had another day to revise for tomor...  negative   \n",
       "1007587  Besides his books, he has written for numerous...   neutral   \n",
       "\n",
       "            source  \n",
       "447463    stanford  \n",
       "1062506   stanford  \n",
       "283133    stanford  \n",
       "688908   wikipedia  \n",
       "686414    stanford  \n",
       "986570    stanford  \n",
       "899448    stanford  \n",
       "638663   wikipedia  \n",
       "737824    stanford  \n",
       "1007587  wikipedia  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master = (smr_df.append(airlines_df)\n",
    "          .append(yelp_df)\n",
    "          .append(amazon_df)\n",
    "          .append(stanford_df.sample(frac=1).reset_index(drop=True)[:int(len(stanford_df)/2)])\n",
    "          .append(wiki_sample)\n",
    "          .reset_index(drop=True))\n",
    "\n",
    "print(master.shape)\n",
    "print(master.source.value_counts())\n",
    "# shuffle dataset\n",
    "master = master.sample(frac=1).reset_index(drop=True)\n",
    "master.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    414895\n",
       "positive    409308\n",
       "neutral     403099\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the dataset is imbalanced but let's go ahead and try to clean it all right now using nltk tweet tokenizer and then tune the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_punctuations(row):\n",
    "#     return re.sub(r'[^\\w\\s]','',row)\n",
    "\n",
    "# Let's leave the punctuations in there for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>source</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>344154</th>\n",
       "      <td>According to the photographer of this photogra...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>[according, photographer, photograph, ,, also,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307513</th>\n",
       "      <td>will not sleep early tonight.. i LOVE programm...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "      <td>[sleep, early, tonight, .., love, programming,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833636</th>\n",
       "      <td>Why can I not sleep in anymore despite going t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>stanford</td>\n",
       "      <td>[sleep, anymore, despite, going, bed, 4, hours...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164366</th>\n",
       "      <td>It is very loosely based on the true life stor...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>[loosely, based, true, life, story, newton, kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015286</th>\n",
       "      <td>@tommcfly aww!  poor Harry..!    hahaa, did yo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "      <td>[aww, !, poor, harry, .., !, hahaa, ,, take, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979202</th>\n",
       "      <td>I have a very talented girlfriend</td>\n",
       "      <td>positive</td>\n",
       "      <td>stanford</td>\n",
       "      <td>[talented, girlfriend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538544</th>\n",
       "      <td>He was the only one in his senior class</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>[one, senior, class]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600205</th>\n",
       "      <td>@ home, totally pissed off, don`t wanna hear o...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stanford</td>\n",
       "      <td>[@, home, ,, totally, pissed, ,, `, wanna, hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692659</th>\n",
       "      <td>Development began in 2011 after the studio com...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>[development, began, 2011, studio, completed, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73199</th>\n",
       "      <td>Released on June 19, 2000 as a lower-cost offe...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>[released, june, 19, ,, 2000, lower-cost, offe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text sentiment  \\\n",
       "344154   According to the photographer of this photogra...   neutral   \n",
       "307513   will not sleep early tonight.. i LOVE programm...  negative   \n",
       "833636   Why can I not sleep in anymore despite going t...  positive   \n",
       "1164366  It is very loosely based on the true life stor...   neutral   \n",
       "1015286  @tommcfly aww!  poor Harry..!    hahaa, did yo...  negative   \n",
       "979202                  I have a very talented girlfriend   positive   \n",
       "538544             He was the only one in his senior class   neutral   \n",
       "600205   @ home, totally pissed off, don`t wanna hear o...  negative   \n",
       "692659   Development began in 2011 after the studio com...   neutral   \n",
       "73199    Released on June 19, 2000 as a lower-cost offe...   neutral   \n",
       "\n",
       "            source                                            cleaned  \n",
       "344154   wikipedia  [according, photographer, photograph, ,, also,...  \n",
       "307513    stanford  [sleep, early, tonight, .., love, programming,...  \n",
       "833636    stanford  [sleep, anymore, despite, going, bed, 4, hours...  \n",
       "1164366  wikipedia  [loosely, based, true, life, story, newton, kn...  \n",
       "1015286   stanford  [aww, !, poor, harry, .., !, hahaa, ,, take, n...  \n",
       "979202    stanford                             [talented, girlfriend]  \n",
       "538544   wikipedia                               [one, senior, class]  \n",
       "600205    stanford  [@, home, ,, totally, pissed, ,, `, wanna, hea...  \n",
       "692659   wikipedia  [development, began, 2011, studio, completed, ...  \n",
       "73199    wikipedia  [released, june, 19, ,, 2000, lower-cost, offe...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = nltk.casual.TweetTokenizer(strip_handles=True, \n",
    "                                       preserve_case=False,\n",
    "                                       reduce_len=True) \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.stem import PorterStemmer # let's not stem the words\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# remove stop words\n",
    "master['cleaned'] = master.text.apply(lambda x: [w for w in tokenizer.tokenize(x) if not w in stop_words])\n",
    "master.sample(10)                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rerun models\n",
    "\n",
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = master.text, master.sentiment\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = nltk.casual.TweetTokenizer(strip_handles=True, \n",
    "                                       preserve_case=False,\n",
    "                                       reduce_len=True) \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "count_vect = CountVectorizer(tokenizer=tokenizer.tokenize) \n",
    "classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = Pipeline([\n",
    "        ('vectorizer', count_vect),\n",
    "        ('classifier', classifier)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_confusion import ConfusionMatrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def genericize_mentions(text):\n",
    "    return re.sub(r'@[\\w_-]+', 'thisisanatmention', text)\n",
    "\n",
    "def get_tweet_length(text):\n",
    "    return len(text)\n",
    "\n",
    "def pipelinize(function, active=True):\n",
    "    def list_comprehend_a_function(list_or_series, active=True):\n",
    "        if active:\n",
    "            return [function(i) for i in list_or_series]\n",
    "        else: # if it's not active, just pass it right back\n",
    "            return list_or_series\n",
    "    return FunctionTransformer(list_comprehend_a_function, validate=False, kw_args={'active':active})\n",
    "\n",
    "def reshape_a_feature_column(series):\n",
    "    return np.reshape(np.asarray(series), (len(series), 1))\n",
    "\n",
    "def pipelinize_feature(function, active=True):\n",
    "    def list_comprehend_a_function(list_or_series, active=True):\n",
    "        if active:\n",
    "            processed = [function(i) for i in list_or_series]\n",
    "            processed = reshape_a_feature_column(processed)\n",
    "            return processed\n",
    "#         This is incredibly stupid and hacky, but we need it to do a grid search.\n",
    "#         If a feature is deactivated, we're going to just return a column of zeroes.\n",
    "#         Zeroes shouldn't affect the regression, but other values may.\n",
    "#         If you really want brownie points, consider pulling out that feature column later in the pipeline.\n",
    "        else:\n",
    "            return reshape_a_feature_column(np.zeros(len(list_or_series)))\n",
    "\n",
    "    return FunctionTransformer(list_comprehend_a_function, validate=False, kw_args={'active':active})\n",
    "\n",
    "def display_null_accuracy(y_test):\n",
    "    value_counts = pd.value_counts(y_test)\n",
    "    null_accuracy = max(value_counts) / float(len(y_test))\n",
    "    print('null accuracy: %s' % '{:.2%}'.format(null_accuracy))\n",
    "    return null_accuracy\n",
    "\n",
    "def display_accuracy_score(y_test, y_pred_class):\n",
    "    score = accuracy_score(y_test, y_pred_class)\n",
    "    print('accuracy score: %s' % '{:.2%}'.format(score))\n",
    "    return score\n",
    "\n",
    "def display_accuracy_difference(y_test, y_pred_class):\n",
    "    null_accuracy = display_null_accuracy(y_test)\n",
    "    accuracy_score = display_accuracy_score(y_test, y_pred_class)\n",
    "    difference = accuracy_score - null_accuracy\n",
    "    if difference > 0:\n",
    "        print('model is %s more accurate than null accuracy' % '{:.2%}'.format(difference))\n",
    "    elif difference < 0:\n",
    "        print('model is %s less accurate than null accuracy' % '{:.2%}'.format(abs(difference)))\n",
    "    elif difference == 0:\n",
    "        print('model is exactly as accurate as null accuracy')\n",
    "    return null_accuracy, accuracy_score\n",
    "\n",
    "def train_test_and_evaluate(pipeline, X_train, y_train, X_test, y_test):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred_class = pipeline.predict(X_test)\n",
    "    confusion_matrix = ConfusionMatrix(list(y_test), list(y_pred_class))\n",
    "    display_accuracy_difference(y_test, y_pred_class)\n",
    "    print('-' * 75 + '\\nConfusion Matrix\\n')\n",
    "    print(confusion_matrix)\n",
    "    print('-' * 75 + '\\nClassification Report\\n')\n",
    "    print(metrics.classification_report(y_test, y_pred_class))\n",
    "      \n",
    "    return pipeline, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null accuracy: 33.78%\n",
      "accuracy score: 85.48%\n",
      "model is 51.70% more accurate than null accuracy\n",
      "---------------------------------------------------------------------------\n",
      "Confusion Matrix\n",
      "\n",
      "Predicted  negative  neutral  positive  __all__\n",
      "Actual                                         \n",
      "negative      81153      845     21656   103654\n",
      "neutral         666    98843       989   100498\n",
      "positive      19247     1142     82285   102674\n",
      "__all__      101066   100830    104930   306826\n",
      "---------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.78      0.79    103654\n",
      "     neutral       0.98      0.98      0.98    100498\n",
      "    positive       0.78      0.80      0.79    102674\n",
      "\n",
      "   micro avg       0.85      0.85      0.85    306826\n",
      "   macro avg       0.86      0.86      0.86    306826\n",
      "weighted avg       0.85      0.85      0.85    306826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_pipeline, confusion_matrix = train_test_and_evaluate(sentiment_pipeline, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../pickle_files/vectorizer_and_logreg.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(sentiment_pipeline, '../pickle_files/vectorizer_and_logreg.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a custom function to a pipeline\n",
    "\n",
    "TODO: Start here: https://ryan-cranfill.github.io/sentiment-pipeline-sklearn-3/\n",
    "\n",
    "What features might make sense to add? Do we want to add tweet length? Should we just stick with our text vectorizer for now? I say stick with what we have for now and see how it performs, we can iterate later if we want to. MVP MVP MVP!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_length(text):\n",
    "    return len(text)\n",
    "\n",
    "def reshape_a_feature_column(series):\n",
    "    return np.reshape(np.asarray(series), (len(series), 1))\n",
    "\n",
    "def pipelinize_feature(function, active=True):\n",
    "    def list_comprehend_a_function(list_or_series, active=True):\n",
    "        if active:\n",
    "            processed = [function(i) for i in list_or_series]\n",
    "            processed = reshape_a_feature_column(processed)\n",
    "            return processed\n",
    "#         This is incredibly stupid and hacky, but we need it to do a grid search with activation/deactivation.\n",
    "#         If a feature is deactivated, we're going to just return a column of zeroes.\n",
    "#         Zeroes shouldn't affect the regression, but other values may.\n",
    "#         If you really want brownie points, consider pulling out that feature column later in the pipeline.\n",
    "        else:\n",
    "            return reshape_a_feature_column(np.zeros(len(list_or_series)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null accuracy: 33.78%\n",
      "accuracy score: 85.61%\n",
      "model is 51.83% more accurate than null accuracy\n",
      "---------------------------------------------------------------------------\n",
      "Confusion Matrix\n",
      "\n",
      "Predicted  negative  neutral  positive  __all__\n",
      "Actual                                         \n",
      "negative      81208      823     21623   103654\n",
      "neutral         637    99032       829   100498\n",
      "positive      19217     1017     82440   102674\n",
      "__all__      101062   100872    104892   306826\n",
      "---------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.78      0.79    103654\n",
      "     neutral       0.98      0.99      0.98    100498\n",
      "    positive       0.79      0.80      0.79    102674\n",
      "\n",
      "   micro avg       0.86      0.86      0.86    306826\n",
      "   macro avg       0.86      0.86      0.86    306826\n",
      "weighted avg       0.86      0.86      0.86    306826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "\n",
    "\n",
    "sentiment_pipeline = Pipeline([\n",
    "        ('genericize_mentions', pipelinize(genericize_mentions, active=True)),\n",
    "        ('features', FeatureUnion([\n",
    "                    ('vectorizer', count_vect),\n",
    "                    ('post_length', pipelinize_feature(get_tweet_length, active=True))\n",
    "                ])),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "\n",
    "sentiment_pipeline, confusion_matrix = train_test_and_evaluate(sentiment_pipeline, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV\n",
    "\n",
    "...is not working. try it on a Google Colab notebook using the GPU. Skip it for now cuz i'm being impatient at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tokenizer_lowercase = nltk.casual.TweetTokenizer(preserve_case=False, reduce_len=False)\n",
    "tokenizer_lowercase_reduce_len = nltk.casual.TweetTokenizer(preserve_case=False, reduce_len=True)\n",
    "tokenizer_uppercase = nltk.casual.TweetTokenizer(preserve_case=True, reduce_len=False)\n",
    "tokenizer_uppercase_reduce_len = nltk.casual.TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "\n",
    "# Our parameter dictionary\n",
    "# You access parameters by giving the dictionary keys of <featurename>__<parameter>\n",
    "# The values of each keys are a list of values that you want to test\n",
    "\n",
    "parameters = {\n",
    "    'genericize_mentions__kw_args': [{'active':False}, {'active':True}], # genericizing mentions on/off\n",
    "    'features__vectorizer__ngram_range': [(1,1), (1,2)], # ngram range of tokenizer\n",
    "    'features__vectorizer__tokenizer': [tokenizer_lowercase.tokenize, # differing parameters for the TweetTokenizer\n",
    "                                        tokenizer_lowercase_reduce_len.tokenize,\n",
    "                                        tokenizer_uppercase.tokenize,\n",
    "                                        tokenizer_uppercase_reduce_len.tokenize,\n",
    "                                        None], # None will use the default tokenizer\n",
    "    'features__vectorizer__max_df': [0.25, 0.5], # maximum document frequency for the CountVectorizer\n",
    "    'classifier__C': np.logspace(-1, 0, 1) # C value for the LogisticRegression\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(sentiment_pipeline, parameters, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e5d2ae4f6e97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-9f6ea7f21ff0>\u001b[0m in \u001b[0;36mtrain_test_and_evaluate\u001b[0;34m(pipeline, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_test_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0my_pred_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfusionMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    229\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, **fit_params)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    791\u001b[0m             delayed(_fit_transform_one)(trans, X, y, weight,\n\u001b[1;32m    792\u001b[0m                                         **fit_params)\n\u001b[0;32m--> 793\u001b[0;31m             for name, trans, weight in self._iter())\n\u001b[0m\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, **fit_params)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1032\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    326\u001b[0m                                                tokenize)\n\u001b[1;32m    327\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 328\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk/tokenize/casual.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0msafe_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHANG_RE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\1\\1\\1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;31m# Tokenize:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWORD_RE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;31m# Possibly alter the case, but avoid changing emoticons like :D into :d:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_case\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid, confusion_matrix = train_test_and_evaluate(grid, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TRY DOING THE ABOVE IN A GOOGLE NOTEBOOK - IT MIGHT BE FASTER WITH MULTI PROCESSING.\n",
    "\n",
    "### This is already saved to a pickle file. So let's do the same to the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.casual.TweetTokenizer(strip_handles=True, \n",
    "                                       preserve_case=False,\n",
    "                                       reduce_len=True) \n",
    "\n",
    "tdidf_vect = TfidfVectorizer(ngram_range=(1, 2),\n",
    "                             max_features=5000,\n",
    "                             tokenizer=tokenizer.tokenize)\n",
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "\n",
    "sentiment_mnb_pipeline = Pipeline([\n",
    "    ('vectorizer', tdidf_vect),\n",
    "    ('classifier', mnb_classifier)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null accuracy: 33.78%\n",
      "accuracy score: 82.72%\n",
      "model is 48.94% more accurate than null accuracy\n",
      "---------------------------------------------------------------------------\n",
      "Confusion Matrix\n",
      "\n",
      "Predicted  negative  neutral  positive  __all__\n",
      "Actual                                         \n",
      "negative      80761     1635     21258   103654\n",
      "neutral        1078    98259      1161   100498\n",
      "positive      25452     2438     74784   102674\n",
      "__all__      107291   102332     97203   306826\n",
      "---------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.78      0.77    103654\n",
      "     neutral       0.96      0.98      0.97    100498\n",
      "    positive       0.77      0.73      0.75    102674\n",
      "\n",
      "   micro avg       0.83      0.83      0.83    306826\n",
      "   macro avg       0.83      0.83      0.83    306826\n",
      "weighted avg       0.83      0.83      0.83    306826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_mnb_pipeline, confusion_matrix = train_test_and_evaluate(sentiment_mnb_pipeline, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../pickle_files/vectorizer_and_mnb.pkl']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pickle the pipeline\n",
    "joblib.dump(sentiment_mnb_pipeline, '../pickle_files/vectorizer_and_mnb.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SGD Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "tokenizer = nltk.casual.TweetTokenizer(strip_handles=True, \n",
    "                                       preserve_case=False,\n",
    "                                       reduce_len=True) \n",
    "\n",
    "tdidf_vect = TfidfVectorizer(ngram_range=(1, 2),\n",
    "                             max_features=5000,\n",
    "                             tokenizer=tokenizer.tokenize)\n",
    "\n",
    "sgd_model = SGDClassifier(max_iter=1000)\n",
    "\n",
    "sentiment_sgd_pipeline = Pipeline([\n",
    "    ('vectorizer', tdidf_vect),\n",
    "    ('classifier', sgd_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null accuracy: 33.78%\n",
      "accuracy score: 83.32%\n",
      "model is 49.53% more accurate than null accuracy\n",
      "---------------------------------------------------------------------------\n",
      "Confusion Matrix\n",
      "\n",
      "Predicted  negative  neutral  positive  __all__\n",
      "Actual                                         \n",
      "negative      80007     2257     21390   103654\n",
      "neutral         677    99076       745   100498\n",
      "positive      23041     3079     76554   102674\n",
      "__all__      103725   104412     98689   306826\n",
      "---------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77    103654\n",
      "     neutral       0.95      0.99      0.97    100498\n",
      "    positive       0.78      0.75      0.76    102674\n",
      "\n",
      "   micro avg       0.83      0.83      0.83    306826\n",
      "   macro avg       0.83      0.83      0.83    306826\n",
      "weighted avg       0.83      0.83      0.83    306826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_sgd_pipeline, confusion_matrix = train_test_and_evaluate(sentiment_sgd_pipeline, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../pickle_files/vectorizer_and_sgd.pkl']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pickle the pipeline\n",
    "joblib.dump(sentiment_sgd_pipeline, '../pickle_files/vectorizer_and_sgd.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply to the real world notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
